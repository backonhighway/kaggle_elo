class RankLgb(GoldenLgb):
    def __init__(self):
        super().__init__()
        self.train_param = {
            'num_leaves': 31,
            'min_data_in_leaf': 30,
            'objective': 'lambdarank',
            'max_depth': -1,
            'learning_rate': 0.01,
            "boosting": "gbdt",
            "feature_fraction": 0.9,
            "bagging_freq": 1,
            "bagging_fraction": 0.9,
            "bagging_seed": 11,
            "metric": 'rmse',
            "lambda_l1": 0.1,
            "verbosity": -1,
            "random_state": 4590,
        }

    def do_train_direct(self, x_train, x_test, y_train, y_test):
        print("start train")
        batch_size = 10
        t_len = x_train.shape[0]
        g_train = [batch_size for _ in range(t_len // batch_size)]
        if t_len % batch_size != 0:
            g_train = g_train + [t_len % batch_size]

        t_len = x_test.shape[0]
        g_test = [batch_size for _ in range(t_len // batch_size)]
        if t_len % batch_size != 0:
            g_test = g_test + [t_len % batch_size]

        # print(g_train)
        # print(g_test)
        lgb_train = lgb.Dataset(x_train, y_train, group=g_train)
        lgb_eval = lgb.Dataset(x_test, y_test, group=g_test)

        model = lgb.train(self.train_param,
                          lgb_train,
                          valid_sets=[lgb_eval],
                          verbose_eval=1000,
                          num_boost_round=3000,
                          early_stopping_rounds=100,
                          categorical_feature=self.category_col)
        return model